---
layout: post
title:  "GBDT"
date:   2024-08-30 14:45:00 +0800
author: dennis-yew
tag: Life
---
# GBDT 梯度提升树

梯度提升树的原理与残差拟合类似，就是先通过一棵树对模型进行拟合，拟合出来的结果与实际观测值会有残差。我们继续训练一棵新的树，去拟合这个残差。然后，我们将新树的预测结果与原来的模型预测结果相加，得到一个新的、更准确的预测结果。这里我给一个一元回归的残差分析例子。

![残差图](/assets/img/2024-08-30-2.png)

## 决策树的基本原理

本质思维，考虑什么最重要，下一个是什么，也就是给所有的因子划分重要的顺序，这和赋予权重还不太一样。

决策树在选择每一层因子的标准是基尼系数最小，为什么？

决策树算法中常用的一个不纯度指标。当我们在构建决策树时，会不断地对数据进行分割，以找到最佳的分裂点。而基尼系数最小，就意味着分裂后的数据集纯度最高，即同一类别的数据被分到了一起。


## 简单线性回归的残差分析

简单线性回归有两个基本假设：

1. 符合 \(A + B \cdot X + e\)
2. \(e\) 的残差服从均值为零，方差为一个固定值的正态分布：\(N(0, \sigma^2)\)

所谓的残差，就是预测值与观测值之间的差，也是公式中的 \(e\)。因此残差检验的意义在于检测第二个假设，查看是否符合均值为零的正态分布。

至于如何检验残差服从正态分布，常见的方式是通过残差图来观察。如果残差的分布介于两条平行线之间，且均值大致为零，不随自变量 \(X\) 移动，那么我们认为其符合正态分布。如果残差图显示其会随 \(X\) 移动，或呈现凹型或凸型，则说明模型假设的 \(e\) 有错误，需要更改为多元回归或曲线回归模型。

由于残差符合正态分布，我们可以将其标准化，用残差除以估计的标准差来对其进行标准化。标准化后的数据应显示在残差图中，且至少 95% 的点必须分布在 +2 和 -2 之间。
![残差分析例子](/assets/img/2024-08-30-1.png)


## CatBoost

CatBoost是一个基于梯度提升树的项目，支持 C、R、Python 语言。作为一名深度使用该项目的开发者，本文将详细叙述我学习该项目的历程，并分享如何将它应用于解决实际问题。

决策树在很多情况下能够取得不错的预测效果，它可能轻松达到 70-80% 的预测准确率，体现的是分而治之的思想。在神经网络盛行的今天，重新审视决策树的价值，算法没有优劣之分，关键在于解决特定问题。

# Kaggle

Kaggle 是著名的数据科学学习平台，拥有海量来自各行各业的公开数据集，定期举办各种主题的机器学习竞赛。

## 构建和使用模型的步骤

1. **定义**：它将是什么类型的模型？决策树？其他类型的模型？还指定了模型类型的其他一些参数。
2. **拟合**：从提供的数据中捕获模式。这是建模的核心。
3. **预测**：预测结果。
4. **评估**：确定模型预测的准确性。