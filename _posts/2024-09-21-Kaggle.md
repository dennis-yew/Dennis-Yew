---
layout: post
title:  "Kaggle"
date:   2024-09-21 12:37:00 +0800
author: dennis-yew
tag: Tech
---
## Machine Learning

### 常用语法
{% highlight python %}
import pandas as pd
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path)
melbourne_data.columns # 展示变量名
{% endhighlight %}

{% highlight python %}
melbourne_data = melbourne_data.dropna(axis=0)

{% endhighlight %}
dropna(axis=0)： 是一个 Pandas DataFrame 的方法，用于删除包含缺失值（NaN）的行或列。
axis=0： 指定删除的行。如果设置为 axis=1，则删除的是列。

{% highlight python %}
X.describe() # 简单描述性分析：最大最小值均值等
X.head() # 展示头7行
{% endhighlight %}

### 决策树
关于决策树欠拟合与过拟合的问题，详见GBDT

{% highlight python %}
# Import helpful libraries

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split

# Load the data, and separate the target
iowa_file_path = '../input/train.csv'
home_data = pd.read_csv(iowa_file_path)
y = home_data.SalePrice

# Create X (After completing the exercise, you can return to modify this line!)
features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']

# Select columns corresponding to features, and preview the data
X = home_data[features]
X.head()

# Split into validation and training data
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) # 固定随机数，使得每次测试的结果相同

# Define a random forest model
rf_model = RandomForestRegressor(random_state=1)
rf_model.fit(train_X, train_y)
rf_val_predictions = rf_model.predict(val_X)
rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)

print("Validation MAE for Random Forest Model: {:,.0f}".format(rf_val_mae))
{% endhighlight %}





As mentioned above, some of the features will throw an error if you try to use them to train your model. The **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.

The **[Pandas](https://kaggle.com/Learn/Pandas)** course will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects.

You are also ready for the **[Deep Learning](https://kaggle.com/Learn/intro-to-Deep-Learning)** course, where you will build models with better-than-human level performance at computer vision tasks.
